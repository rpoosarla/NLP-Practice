{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Sentiment Analysis using TF-IDF model and Linear SVC\n",
    "Data: \n",
    "out of 50000 movie reviews 40000 reviews have been used to train and rest of the\n",
    "10000 reviews have been used to test the model performance.\n",
    "Preprocessing:\n",
    "1.removing non-character symbols, \n",
    "2. add 'not_' before the word where there's an occurence of 'not'.\n",
    "Model:\n",
    "Here I have used TfidfVectorizer from sklearn.feature_extraction.text\n",
    "we are not using bag words model as it will not give any importance to model.\n",
    "Machine Learning:\n",
    "There are 2000 features used in this model and because of that\n",
    "LinearSVC works the best for predicting sentiments with best accuracy.\n",
    "This model is also tested with other learning algorithms like\n",
    "MultinomialNB, RandomForest with 500 estimators and LinearSVC out-performs\n",
    "all of them.\n",
    "Cross Validation:\n",
    "We have used 10 fold cross validation on this model to determine the\n",
    "accuracy of the model on different datasets and we get a mean of 89.80%\n",
    "from all the 10 results.\n",
    "Complete list of Accuracies per max_feature value:\n",
    "1000 86.50%\n",
    "2000 88.27%\n",
    "3000 88.87%\n",
    "4000 89.17%\n",
    "5000 89.28%\n",
    "6000 89.42%\n",
    "7000 89.61%\n",
    "8000 89.62%\n",
    "9000 89.74%\n",
    "10000 89.74%\n",
    "12000 89.80%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rajkumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d794c956025c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Importing the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mreviews_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mreviews_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreviews_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreviews_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\datasets\\base.py\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     folders = [f for f in sorted(listdir(container_path))\n\u001b[0m\u001b[0;32m    167\u001b[0m                if isdir(join(container_path, f))]\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train/'"
     ]
    }
   ],
   "source": [
    "Skip to content\n",
    " \n",
    "Search or jump to…\n",
    "\n",
    "Pull requests\n",
    "Issues\n",
    "Marketplace\n",
    "Explore\n",
    " \n",
    "@rpoosarla \n",
    "0\n",
    "0 0 rpoosarla/NLP Private\n",
    " Code  Issues 0  Pull requests 0  Projects 0  Security  Insights  Settings\n",
    "NLP/Section 7 - Text Classification/Bonus - IMDB sentiment analysis.py\n",
    "@bijoyandas bijoyandas Add files via upload\n",
    "1dc2605 on May 5, 2018\n",
    "157 lines (124 sloc)  4.79 KB\n",
    "    \n",
    "# Created by Bijoyan Das on 12/2/2018 at 12:33:43 PM\n",
    "\"\"\"\n",
    "Sentiment Analysis using TF-IDF model and Linear SVC\n",
    "Data: \n",
    "50000 movie reviews from IMDB has been used in this model,\n",
    "out of which 40000 reviews have been used to train and rest of the\n",
    "10000 reviews have been used to test the model performance.\n",
    "Preprocessing:\n",
    "Preprocessing stage consists of removing non-character symbols, \n",
    "adding 'not_' before the word where there's an occurence of 'not' etc.\n",
    "The preprocessed data is stored in corpus.\n",
    "Model:\n",
    "Here I have used TfidfVectorizer from sklearn.feature_extraction.text\n",
    "library which works similarly to CounterVectorizer followed by\n",
    "TfidfTransformer. Due to this we can assign different weights to words,\n",
    "and all words are not given the same importance.\n",
    "Machine Learning:\n",
    "There are 2000 features used in this model and because of that\n",
    "LinearSVC works the best for predicting sentiments with a accuracy of\n",
    "89.80%. This model is also tested with other learning algorithms like\n",
    "MultinomialNB, RandomForest with 500 estimators and LinearSVC out-performs\n",
    "all of them.\n",
    "Cross Validation:\n",
    "We have used 10 fold cross validation on this model to determine the\n",
    "accuracy of the model on different datasets and we get a mean of 89.80%\n",
    "from all the 10 results.\n",
    "Complete list of Accuracies per max_feature value:\n",
    "1000 86.50%\n",
    "2000 88.27%\n",
    "3000 88.87%\n",
    "4000 89.17%\n",
    "5000 89.28%\n",
    "6000 89.42%\n",
    "7000 89.61%\n",
    "8000 89.62%\n",
    "9000 89.74%\n",
    "10000 89.74%\n",
    "12000 89.80%\n",
    "\"\"\"\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Importing the dataset\n",
    "from sklearn.datasets import load_files\n",
    "reviews_train = load_files('train/')\n",
    "reviews_test = load_files('test/')\n",
    "X_train,y_train = reviews_train.data,reviews_train.target\n",
    "X_test,y_test = reviews_test.data,reviews_test.target\n",
    "X = X_train + X_test\n",
    "y = np.concatenate([y_train,y_test])\n",
    "X = X[:50000]\n",
    "y = y[:50000]\n",
    "\n",
    "\n",
    "# Unpickling dataset\n",
    "X_in = open('X.pickle','rb')\n",
    "y_in = open('y.pickle','rb')\n",
    "X = pickle.load(X_in)\n",
    "y = pickle.load(y_in)\n",
    "\n",
    "\n",
    "# Improving the stop words list\n",
    "stop_words = stopwords.words('english')\n",
    "uncheck_words = ['don','won','doesn','couldn','isn','wasn','wouldn','can','ain','shouldn','not']\n",
    "\n",
    "\n",
    "# Creating the corpus which is the input to TfidfVectorizer\n",
    "corpus = []\n",
    "for i in range(0, len(X)):\n",
    "    antonyms = []\n",
    "    review = re.sub(r'\\W', ' ', str(X[i]))\n",
    "    review = re.sub(r'\\d', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = re.sub(r'br[\\s$]', ' ', review)\n",
    "    review = re.sub(r'\\s+[a-z][\\s$]', ' ',review)\n",
    "    review = re.sub(r'b\\s+', '', review)\n",
    "    review = re.sub(r'\\s+', ' ', review)\n",
    "    word_list = review.split(' ')\n",
    "    newword_list = []\n",
    "    temp_word = ''\n",
    "    for word in word_list:\n",
    "        if temp_word in uncheck_words:\n",
    "            if word not in stop_words:\n",
    "                word = 'not_' + word\n",
    "                temp_word = ''\n",
    "        if word in uncheck_words:\n",
    "            temp_word = word\n",
    "        if word not in uncheck_words:\n",
    "            newword_list.append(word)\n",
    "    review = ' '.join(newword_list)\n",
    "    corpus.append(review)    \n",
    "\n",
    "# Creating the weighted BOW model using TF-IDF methodology\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tiv = TfidfVectorizer(max_features = 8000, min_df = 2, norm=\"l2\", use_idf=True, sublinear_tf = True, max_df = 0.6, stop_words = stop_words)\n",
    "X = tiv.fit_transform(corpus).toarray()\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, sent_train, sent_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# Fitting the Training set to Linear SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC(C = 0.1)\n",
    "classifier.fit(text_train,sent_train)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(text_train,sent_train)\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(text_train, sent_train)\n",
    "\n",
    "\n",
    "# Pickling classifier\n",
    "with open('svcclassifier.pickle','wb') as f:\n",
    "    pickle.dump(classifier,f)\n",
    "    \n",
    "\n",
    "# Pikling TF-IDF model\n",
    "with open('TFIDF.pickle','wb') as f:\n",
    "    pickle.dump(tiv,f)    \n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "sent_pred = classifier.predict(text_test)\n",
    "\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(sent_test, sent_pred)\n",
    "\n",
    "print(cm[0][0]+cm[1][1])\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = text_train, y = sent_train, cv = 10)\n",
    "accuracies.mean()\n",
    "accuracies.std()\n",
    "© 2019 GitHub, Inc.\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Help\n",
    "Contact GitHub\n",
    "Pricing\n",
    "API\n",
    "Training\n",
    "Blog\n",
    "About\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
